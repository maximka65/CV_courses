{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "Напомню, свертки - это то, с чего начался хайп нейронных сетей в районе 2012-ого.\n",
    "\n",
    "Работают они примерно так:  \n",
    "![Conv example](https://image.ibb.co/e6t8ZK/Convolution.gif)   \n",
    "From [Feature extraction using convolution](http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution).\n",
    "\n",
    "Формально - учатся наборы фильтров, каждый из которых скалярно умножается на элементы матрицы признаков. На картинке выше исходная матрица сворачивается с фильтром\n",
    "$$\n",
    " \\begin{pmatrix}\n",
    "  1 & 0 & 1 \\\\\n",
    "  0 & 1 & 0 \\\\\n",
    "  1 & 0 & 1\n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Но нужно не забывать, что свертки обычно имеют ещё такую размерность, как число каналов. Например, картинки имеют обычно три канала: RGB.  \n",
    "Наглядно демонстрируется как выглядят при этом фильтры [здесь](http://cs231n.github.io/convolutional-networks/#conv).\n",
    "\n",
    "После сверток обычно следуют pooling-слои. Они помогают уменьшить размерность тензора, с которым приходится работать. Самым частым является max-pooling:  \n",
    "![maxpooling](http://cs231n.github.io/assets/cnn/maxpool.jpeg)  \n",
    "From [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/#pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Свёртки для текстов\n",
    "\n",
    "Для текстов свертки работают как n-граммные детекторы (примерно). Каноничный пример символьной сверточной сети:\n",
    "\n",
    "![text-convs](https://image.ibb.co/bC3Xun/2018_03_27_01_24_39.png)  \n",
    "From [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)\n",
    "\n",
    "*Сколько учится фильтров на данном примере?*\n",
    "\n",
    "На картинке показано, как из слова извлекаются 2, 3 и 4-граммы. Например, желтые - это триграммы. Желтый фильтр прикладывают ко всем триграммам в слове, а потом с помощью global max-pooling извлекают наиболее сильный сигнал.\n",
    "\n",
    "Что это значит, если конкретнее?\n",
    "\n",
    "Каждый символ отображается с помощью эмбеддингов в некоторый вектор. А их последовательности - в конкатенации эмбеддингов.  \n",
    "Например, \"abs\" $\\to [v_a; v_b; v_s] \\in \\mathbb{R}^{3 d}$, где $d$ - размерность эмбеддинга. Желтый фильтр $f_k$ имеет такую же размерность $3d$.  \n",
    "Его прикладывание - это скалярное произведение $\\left([v_a; v_b; v_s] \\odot f_k \\right) \\in \\mathbb R$ (один из желтых квадратиков в feature map для данного фильтра).\n",
    "\n",
    "Max-pooling выбирает $max_i \\left( [v_{i-1}; v_{i}; v_{i+1}] \\odot f_k \\right)$, где $i$ пробегается по всем индексам слова от 1 до $|w| - 1$ (либо по большему диапазону, если есть padding'и).   \n",
    "Этот максимум соответствует той триграмме, которая наиболее близка к фильтру по косинусному расстоянию.\n",
    "\n",
    "В результате в векторе после max-pooling'а закодирована информация о том, какие из n-грамм встретились в слове: если встретилась близкая к нашему $f_k$ триграмма, то в $k$-той позиции вектора будет стоять большое значение, иначе - маленькое.\n",
    "\n",
    "А учим мы как раз фильтры. То есть сеть должна научиться определять, какие из n-грамм значимы, а какие - нет.\n",
    "\n",
    "\n",
    "## Основные параметры сверток\n",
    "\n",
    "### Stride\n",
    "\n",
    "Stride - это \"шаг\" с которым идем по матрице объекта\n",
    "\n",
    "<img src=\"strides.gif\">\n",
    "\n",
    "### Dilation\n",
    "\n",
    "Dilation - расстояние между точками ядра\n",
    "\n",
    "<img src=\"dilations.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximcucer/opt/anaconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:285: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR10(root='data/', train=True, download=True)\n",
    "\n",
    "def train_valid_split(Xt):\n",
    "    X_train, X_test = train_test_split(Xt, test_size=0.05, random_state=13)\n",
    "    return X_train, X_test\n",
    "\n",
    "class MyOwnCifar(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, init_dataset, transform=None):\n",
    "        self._base_dataset = init_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self._base_dataset[idx][0]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, self._base_dataset[idx][1]\n",
    "    \n",
    "trans_actions = transforms.Compose([transforms.Scale(44),\n",
    "                                    transforms.RandomCrop(32, padding=4), \n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "train_dataset, valid_dataset = train_valid_split(dataset)\n",
    "\n",
    "train_dataset = MyOwnCifar(train_dataset, trans_actions)\n",
    "valid_dataset = MyOwnCifar(valid_dataset, transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=3)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (dp_three): Dropout(p=0.2, inplace=False)\n",
      "  (dp_four): Dropout(p=0.2, inplace=False)\n",
      "  (bn_one): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_one): Conv2d(3, 30, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn_two): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_two): Conv2d(30, 60, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn_three): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv_three): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn_four): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=480, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.dp_three = nn.Dropout(0.2)\n",
    "        self.dp_four = nn.Dropout(0.2)\n",
    "        \n",
    "        self.bn_one = torch.nn.BatchNorm2d(3) \n",
    "        self.conv_one = torch.nn.Conv2d(3, 30, 3)\n",
    "        self.bn_two = torch.nn.BatchNorm2d(30) \n",
    "        self.conv_two = torch.nn.Conv2d(30, 60, 3)\n",
    "        self.bn_three = torch.nn.BatchNorm2d(60)\n",
    "        self.conv_three = torch.nn.Conv2d(60, 120, 3)\n",
    "        self.bn_four = torch.nn.BatchNorm2d(120)\n",
    "        self.fc1 = torch.nn.Linear(480, 200)\n",
    "        self.fc2 = torch.nn.Linear(200, 60)\n",
    "        self.out = torch.nn.Linear(60, 10)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn_one(x)\n",
    "        x = self.conv_one(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.bn_two(x)\n",
    "        x = self.conv_two(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.bn_three(x)\n",
    "        x = self.conv_three(x)\n",
    "        x = F.leaky_relu(x, 0.1)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.bn_four(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dp_three(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp_four(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return self.out(x)\n",
    "       \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):  \n",
    "    net.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    net.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y in valid_loader:\n",
    "        output = net(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN проблемы:\n",
    "\n",
    "1. Проблема сложность обобщения картинки\n",
    "\n",
    "Первая продвинутая сверточная архитектура - Alexnet\n",
    "\n",
    "<img src=\"alexnet.png\">\n",
    "\n",
    "Развитие - более тяжелые сети VGG\n",
    "\n",
    "<img src=\"vgg.png\">\n",
    "\n",
    "<img src=\"vgg2.jpg\">\n",
    "\n",
    "2. Проблема затухания градиентов - resnet, inception\n",
    "\n",
    "<img src=\"resnet.png\">\n",
    "\n",
    "<img src=\"inception_big.png\">\n",
    "<img src=\"inception.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet18 = models.resnet18()\n",
    "alexnet = models.alexnet()\n",
    "vgg16 = models.vgg16()\n",
    "#inception = models.inception_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/kinetik/.cache/torch/checkpoints/vgg16-397923af.pth\n",
      "\n",
      "\n",
      "  0%|          | 0.00/528M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 184k/528M [00:00<05:19, 1.73MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 608k/528M [00:00<04:56, 1.87MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 1.73M/528M [00:00<03:40, 2.50MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 2.73M/528M [00:00<02:50, 3.24MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 3.80M/528M [00:00<02:13, 4.11MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 4.83M/528M [00:00<01:49, 5.02MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 6.05M/528M [00:00<01:29, 6.14MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 7.30M/528M [00:00<01:14, 7.30MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 8.52M/528M [00:01<01:05, 8.38MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 9.62M/528M [00:01<00:59, 9.06MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 10.9M/528M [00:01<00:53, 10.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 12.2M/528M [00:01<00:50, 10.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 13.5M/528M [00:01<00:46, 11.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 14.7M/528M [00:01<00:45, 11.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 16.0M/528M [00:01<00:44, 12.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 17.4M/528M [00:01<00:42, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▎         | 18.6M/528M [00:01<00:42, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 19.9M/528M [00:01<00:41, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 21.1M/528M [00:02<00:41, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 22.4M/528M [00:02<00:41, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 23.6M/528M [00:02<00:41, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 24.9M/528M [00:02<00:41, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▍         | 26.1M/528M [00:02<00:41, 12.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 27.3M/528M [00:02<00:41, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 28.5M/528M [00:02<00:41, 12.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 29.7M/528M [00:02<00:42, 12.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 31.0M/528M [00:02<00:41, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 32.2M/528M [00:02<00:41, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▋         | 33.4M/528M [00:03<00:41, 12.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 34.7M/528M [00:03<00:41, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 36.0M/528M [00:03<00:39, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 37.2M/528M [00:03<00:40, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 38.4M/528M [00:03<00:39, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 39.7M/528M [00:03<00:39, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 40.9M/528M [00:03<00:39, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 42.2M/528M [00:03<00:39, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 43.4M/528M [00:03<00:39, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 44.6M/528M [00:03<00:40, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▊         | 45.9M/528M [00:04<00:39, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 47.1M/528M [00:04<00:40, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 48.3M/528M [00:04<00:39, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 49.6M/528M [00:04<00:38, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 50.8M/528M [00:04<00:41, 12.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|▉         | 52.1M/528M [00:04<00:40, 12.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 53.4M/528M [00:04<00:39, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 10%|█         | 54.7M/528M [00:04<00:39, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 56.0M/528M [00:04<00:38, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 57.2M/528M [00:05<00:38, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█         | 58.5M/528M [00:05<00:38, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 11%|█▏        | 59.7M/528M [00:05<00:37, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 61.0M/528M [00:05<00:36, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 62.3M/528M [00:05<00:37, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 63.6M/528M [00:05<00:37, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 64.9M/528M [00:05<00:36, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 66.1M/528M [00:05<00:37, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 67.4M/528M [00:05<00:36, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 68.7M/528M [00:05<00:37, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 69.9M/528M [00:06<00:37, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 13%|█▎        | 71.2M/528M [00:06<00:37, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▎        | 72.4M/528M [00:06<00:37, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 73.6M/528M [00:06<00:37, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 74.9M/528M [00:06<00:37, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 14%|█▍        | 76.1M/528M [00:06<00:36, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 77.4M/528M [00:06<00:37, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 78.6M/528M [00:06<00:37, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 79.9M/528M [00:06<00:36, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▌        | 81.1M/528M [00:06<00:36, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 82.4M/528M [00:07<00:35, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 83.7M/528M [00:07<00:35, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▌        | 84.9M/528M [00:07<00:35, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 16%|█▋        | 86.2M/528M [00:07<00:35, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 87.4M/528M [00:07<00:35, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 88.7M/528M [00:07<00:35, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 89.9M/528M [00:07<00:35, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 17%|█▋        | 91.2M/528M [00:07<00:35, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 92.4M/528M [00:07<00:35, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 93.6M/528M [00:07<00:35, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 94.9M/528M [00:08<00:35, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 96.2M/528M [00:08<00:34, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 97.4M/528M [00:08<00:34, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▊        | 98.7M/528M [00:08<00:34, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 99.9M/528M [00:08<00:34, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 101M/528M [00:08<00:34, 12.9MB/s] \u001b[A\u001b[A\n",
      "\n",
      " 19%|█▉        | 102M/528M [00:08<00:34, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 104M/528M [00:08<00:34, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|█▉        | 105M/528M [00:08<00:33, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 106M/528M [00:08<00:33, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 20%|██        | 107M/528M [00:09<00:33, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 109M/528M [00:09<00:33, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 110M/528M [00:09<00:33, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 111M/528M [00:09<00:33, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██▏       | 113M/528M [00:09<00:33, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 114M/528M [00:09<00:33, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 115M/528M [00:09<00:33, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 116M/528M [00:09<00:32, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 22%|██▏       | 118M/528M [00:09<00:32, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 119M/528M [00:09<00:32, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 120M/528M [00:10<00:32, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 121M/528M [00:10<00:58, 7.34MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 123M/528M [00:10<00:49, 8.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 23%|██▎       | 124M/528M [00:10<00:44, 9.51MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 125M/528M [00:10<00:40, 10.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 127M/528M [00:10<00:37, 11.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 128M/528M [00:10<00:35, 11.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 24%|██▍       | 129M/528M [00:11<00:34, 12.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 130M/528M [00:11<00:33, 12.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▍       | 132M/528M [00:11<00:32, 12.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 133M/528M [00:11<00:32, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 25%|██▌       | 134M/528M [00:11<00:31, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 136M/528M [00:11<00:32, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 137M/528M [00:11<00:31, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▌       | 138M/528M [00:11<00:30, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 140M/528M [00:11<00:30, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 141M/528M [00:12<00:31, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 142M/528M [00:12<00:30, 13.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 143M/528M [00:12<00:32, 12.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 27%|██▋       | 145M/528M [00:12<00:31, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 146M/528M [00:12<00:30, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 147M/528M [00:12<00:30, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 149M/528M [00:12<00:30, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 28%|██▊       | 150M/528M [00:12<00:30, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▊       | 151M/528M [00:12<00:29, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 153M/528M [00:12<00:32, 12.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 154M/528M [00:13<00:32, 12.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 155M/528M [00:13<00:31, 12.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 156M/528M [00:13<00:31, 12.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|██▉       | 157M/528M [00:13<00:31, 12.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 159M/528M [00:13<00:30, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 30%|███       | 160M/528M [00:13<00:30, 12.7MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 161M/528M [00:13<00:29, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 162M/528M [00:13<00:29, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███       | 164M/528M [00:13<00:29, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 31%|███▏      | 165M/528M [00:13<00:28, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 166M/528M [00:14<00:28, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 168M/528M [00:14<00:29, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 169M/528M [00:14<00:28, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 170M/528M [00:14<00:28, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 171M/528M [00:14<00:28, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 173M/528M [00:14<00:28, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 174M/528M [00:14<00:28, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 175M/528M [00:14<00:27, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 33%|███▎      | 176M/528M [00:14<00:27, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▎      | 178M/528M [00:14<00:27, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 179M/528M [00:15<00:27, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 180M/528M [00:15<00:27, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 34%|███▍      | 182M/528M [00:15<00:27, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 183M/528M [00:15<00:27, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▍      | 184M/528M [00:15<00:27, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 186M/528M [00:15<00:27, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 187M/528M [00:15<00:27, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 188M/528M [00:15<00:26, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 189M/528M [00:15<00:27, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▌      | 191M/528M [00:16<00:27, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 36%|███▋      | 192M/528M [00:16<00:27, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 193M/528M [00:16<00:26, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 194M/528M [00:16<00:26, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 196M/528M [00:16<00:26, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 37%|███▋      | 197M/528M [00:16<00:26, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 198M/528M [00:16<00:26, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 200M/528M [00:16<00:26, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 201M/528M [00:16<00:26, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 202M/528M [00:16<00:26, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▊      | 203M/528M [00:17<00:25, 13.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 205M/528M [00:17<00:25, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 206M/528M [00:17<00:25, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 39%|███▉      | 207M/528M [00:17<00:25, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 209M/528M [00:17<00:25, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 210M/528M [00:17<00:26, 12.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|███▉      | 211M/528M [00:17<00:25, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 212M/528M [00:17<00:26, 12.3MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 40%|████      | 214M/528M [00:17<00:26, 12.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 215M/528M [00:17<00:25, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 216M/528M [00:18<00:25, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 217M/528M [00:18<00:25, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████▏     | 219M/528M [00:18<00:24, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 220M/528M [00:18<00:24, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 221M/528M [00:18<00:24, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 223M/528M [00:18<00:24, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 42%|████▏     | 224M/528M [00:18<00:24, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 225M/528M [00:18<00:24, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 226M/528M [00:18<00:24, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 228M/528M [00:19<00:24, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 43%|████▎     | 229M/528M [00:19<00:24, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▎     | 230M/528M [00:19<00:24, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 231M/528M [00:19<00:23, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 233M/528M [00:19<00:23, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 234M/528M [00:19<00:23, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 235M/528M [00:19<00:23, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▍     | 236M/528M [00:19<00:23, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 238M/528M [00:19<00:23, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 239M/528M [00:19<00:23, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 45%|████▌     | 240M/528M [00:20<00:23, 13.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 241M/528M [00:20<00:23, 12.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 243M/528M [00:20<00:23, 12.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▌     | 244M/528M [00:20<00:22, 13.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 46%|████▋     | 245M/528M [00:20<00:22, 13.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 247M/528M [00:20<00:22, 13.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 248M/528M [00:20<00:40, 7.26MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 249M/528M [00:21<00:35, 8.28MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 250M/528M [00:21<00:31, 9.27MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 251M/528M [00:21<00:28, 10.0MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 253M/528M [00:21<00:27, 10.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 254M/528M [00:21<00:25, 11.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 48%|████▊     | 255M/528M [00:21<00:28, 9.97MB/s]\u001b[A\u001b[A\n",
      "\n",
      " 49%|████▊     | 256M/528M [00:21<00:22, 12.4MB/s]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b38694d15ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malexnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malexnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvgg16\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#inception = models.inception_v3(pretrained=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36mvgg16\u001b[0;34m(pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplays\u001b[0m \u001b[0ma\u001b[0m \u001b[0mprogress\u001b[0m \u001b[0mbar\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdownload\u001b[0m \u001b[0mto\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_vgg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vgg16'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/models/vgg.py\u001b[0m in \u001b[0;36m_vgg\u001b[0;34m(arch, cfg, batch_norm, pretrained, progress, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         state_dict = load_state_dict_from_url(model_urls[arch],\n\u001b[0;32m---> 93\u001b[0;31m                                               progress=progress)\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash)\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Downloading: \"{}\" to {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m         \u001b[0mhash_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHASH_REGEX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_hash\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m         \u001b[0mdownload_url_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;31m# Note: extractall() defaults to overwrite file if exists. No need to clean up beforehand.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    404\u001b[0m                   unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n\u001b[1;32m    405\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "alexnet = models.alexnet(pretrained=True)\n",
    "vgg16 = models.vgg16(pretrained=True)\n",
    "#inception = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Необходимые трансформации\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "transform = transforms.Compose([transforms.Resize(256), transforms.CenterCrop(224), normalize, transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но в жизни часто нужно решать какую-то свою задачу, а не ImageNet\n",
    "\n",
    "<img src=\"frozen.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "resnet18 = models.resnet18(pretrained=True)\n",
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_parameter_requires_grad(resnet18, True)\n",
    "resnet18.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinetik/anaconda3/lib/python3.7/site-packages/torchvision/transforms/transforms.py:219: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    }
   ],
   "source": [
    "trans_actions = transforms.Compose([transforms.Scale(256),\n",
    "                                    transforms.RandomCrop(224, padding=4),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "valid_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "train_dataset, valid_dataset = train_valid_split(dataset)\n",
    "\n",
    "train_dataset = MyOwnCifar(train_dataset, trans_actions)\n",
    "valid_dataset = MyOwnCifar(valid_dataset, valid_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=True,\n",
    "                          num_workers=3)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,\n",
    "                          batch_size=128,\n",
    "                          shuffle=False,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam(resnet18.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0315, -0.0001,  0.0153,  ...,  0.0238, -0.0297,  0.0255],\n",
       "         [-0.0341,  0.0109,  0.0349,  ...,  0.0210,  0.0429, -0.0148],\n",
       "         [-0.0154,  0.0323,  0.0428,  ..., -0.0014,  0.0359, -0.0092],\n",
       "         ...,\n",
       "         [-0.0047,  0.0355, -0.0369,  ..., -0.0057, -0.0099,  0.0370],\n",
       "         [-0.0158, -0.0191,  0.0054,  ..., -0.0276, -0.0020,  0.0083],\n",
       "         [-0.0160,  0.0170,  0.0321,  ..., -0.0199, -0.0236, -0.0385]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0160, -0.0325,  0.0384, -0.0281,  0.0218, -0.0348,  0.0241, -0.0013,\n",
       "         -0.0439,  0.0167], requires_grad=True)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = []\n",
    "for name,param in resnet18.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam(params_to_update, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):  \n",
    "    resnet18.train()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = resnet18(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    resnet18.eval()\n",
    "    loss_accumed = 0\n",
    "    for X, y in valid_loader:\n",
    "        output = resnet18(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss_accumed += loss\n",
    "    print(\"Epoch {} valid_loss {}\".format(epoch, loss_accumed))\n",
    "\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
